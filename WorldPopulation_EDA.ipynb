{"metadata":{"colab":{"name":"scratchpad","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# World Population (EDA)","metadata":{"id":"Pm3TjzJRJr6T"}},{"cell_type":"markdown","source":"The folllowing is the list of possible EDA steps to prepare data for training a ML model. In this notebook, I address some of them.\n\n## Steps to do:\n1. Imports\n2. Load data\n3. Know/understand data\n  \n  3.1. df.shape\n\n  3.2. df.head(), df.tail(), df.sample(5)\n\n  3.3. df.info()\n\n  3.4. df.describe(),\n\n  3.5. df.select_dtype(include/exclude=\"number\"/\"categorical\")\n\n  3.6. Feature analysis: Correlation, Mutual Information, Statistical tests.\n\n4. Remove duplicates\n5. Null handling\n6. Skewness handling for numerical data\n7. Outlier Handling\n8. Filter data for analysis\n9. Scale the numeric data\n10. Encode the categorical data\n11. Feature engineering\n  \n  11.1. Dimensionality reduction (PCA, tSNE, UMAP)\n  \n  11.2. Manual Featuring","metadata":{"id":"HYSooH2HKEfe"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n","metadata":{"id":"lIYdn1woOS1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/AlexTheAnalyst/PandasYouTubeSeries/main/world_population.csv')","metadata":{"id":"vSCVXxRR6lVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"id":"gWHD1_GF6r3U","outputId":"48986d2e-4457-4fa4-f6d2-dfda2755fb7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"id":"pSRo6sl66swS","outputId":"3128ff2b-02dc-4cd3-b02a-848ec7f5f6fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T # transpose","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(by=\"World Population Percentage\", ascending=False).head(10)","metadata":{"id":"btVYEZ_3gUm8","outputId":"7e3afef8-61b0-406f-9cd9-58c83366773b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify columns with non-numerical data\nnon_numeric_cols = df.select_dtypes(exclude=['number']).columns\nnumeric_cols = df.select_dtypes(include=['number']).columns\n\n# Drop or encode non-numerical columns before imputation\ndf_numeric = df.drop(non_numeric_cols, axis=1)\ndf_cat = df.drop(numeric_cols, axis=1)","metadata":{"id":"Oz0jtp9RGuUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[numeric_cols].corr()","metadata":{"id":"Y7IbzJlugZmR","outputId":"b01a7f67-4e3c-48d7-9dc9-9e0b6487e164"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[numeric_cols].corr(), annot = True)\nplt.rcParams['figure.figsize'] = (20,7)\nplt.show()","metadata":{"id":"ynm4ZdlfgpN7","outputId":"7e69148e-0e6c-4d72-c2fa-5d52a92436ae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Duplicate","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()\ndf.drop_duplicates()","metadata":{"id":"-fkIBipB6voe","outputId":"17a4709d-a9b0-40eb-e581-39cfa4397d2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NUll handling","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()\n\n#-----------------------------------------\ndf1 = df.dropna()\n\n\n#-----------------------------------------\nfrom sklearn.impute import KNNImputer\ndf_numeric2 = KNNImputer().fit_transform(df_numeric)\n\ndf_numeric2 = pd.DataFrame(df_numeric2, columns=df_numeric.columns)\ndf_non_null = pd.concat((df_numeric2, df_cat), axis=1)","metadata":{"id":"5mT8OxRL61JQ","outputId":"c19d0d90-e9ae-48ea-fef6-b0fc771d1a60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Skewness handing","metadata":{}},{"cell_type":"code","source":"def visualize_numerical_distributions(df, exclude_columns='id'):\n\n    # Set up the figure for multiple subplots\n    num_cols = 3  # Number of columns for the subplot grid\n    num_rows = (len(numeric_cols) + num_cols - 1) // num_cols  # Calculate number of rows needed\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows))\n    fig.suptitle('Distribution of Numerical Features', fontsize=16)\n\n    # Flatten axes array for easy iteration\n    axes = axes.flatten()\n\n    # Iterate over each numerical column and create a histogram with KDE\n    for i, col in enumerate(numeric_cols):\n        sns.histplot(df[col], kde=True, ax=axes[i], color=\"skyblue\", element=\"step\", stat=\"density\")\n        axes[i].set_title(f'Distribution of {col}', fontsize=14)\n        axes[i].set_xlabel(col, fontsize=12)\n        axes[i].set_ylabel('Density', fontsize=12)\n\n    # Remove unused axes\n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    # Adjust layout\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the main title space\n    plt.show()\n    \n    \nvisualize_numerical_distributions(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Growth Rate\"].skew()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\ndef robust_skewness_handler(df, threshold=2.0):\n    transformed_df = df.copy()\n    transformation_dict = {}\n\n    for col in df.select_dtypes(include=[np.number]).columns:\n        data = df[col].dropna()\n        skewness = data.skew()\n        \n        if abs(skewness) > threshold:\n            print(f\"Handling skewness for {col} (skewness: {skewness:.2f})\")\n            \n            # Handle zero-inflated or near-zero-inflated data\n            if (data == 0).sum() / len(data) > 0.1:  # If more than 10% zeros\n                non_zero = data[data != 0]\n                if len(non_zero) > 0:\n                    log_transform = np.log1p(non_zero)\n                    transformed_df.loc[data != 0, col] = log_transform\n                    transformation_dict[col] = ('log1p', 'zero-inflated')\n                    print(f\"  Applied Log1p to non-zero values for {col}\")\n                continue\n            \n            # Try Box-Cox transformation\n            min_val = data.min()\n            if min_val <= 0:\n                shift = abs(min_val) + 1\n                shifted_data = data + shift\n            else:\n                shifted_data = data\n            \n            try:\n                transformed_data, lambda_param = stats.boxcox(shifted_data)\n                transformed_df[col] = transformed_data\n                transformation_dict[col] = ('box-cox', lambda_param)\n                print(f\"  Applied Box-Cox to {col} (lambda: {lambda_param:.2f})\")\n            except:\n                # If Box-Cox fails, try other transformations\n                if skewness > 0:  # Right-skewed\n                    if data.max() / data.min() > 1000:  # Very large range\n                        transformed_df[col] = np.log1p(data)\n                        transformation_dict[col] = ('log1p', None)\n                        print(f\"  Applied Log1p to {col}\")\n                    else:\n                        transformed_df[col] = np.sqrt(data)\n                        transformation_dict[col] = ('sqrt', None)\n                        print(f\"  Applied Square Root to {col}\")\n                else:  # Left-skewed\n                    transformed_df[col] = data ** 2\n                    transformation_dict[col] = ('square', None)\n                    print(f\"  Applied Square to {col}\")\n\n    return transformed_df, transformation_dict\n\n# Example usage:\ntransformed_df, transformations = robust_skewness_handler(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_numerical_distributions(transformed_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier handling","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\ndf.boxplot()\nplt.xticks(rotation=90)\nplt.show()","metadata":{"id":"zaS1TxT067JW","outputId":"b727a3bc-8385-4075-85f8-0902469e6974"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_outliers_percentage(df):\n    outlier_counts = {}\n    for column in df.select_dtypes(include=['number']).columns:\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        # Calculate outliers\n        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n        outlier_counts[column] = len(outliers)\n\n    # Print the percentage of outliers for each column\n    for column in outlier_counts:\n        percentage = (outlier_counts[column] / len(df)) * 100\n        print(f\"Percentage of outliers in {column}: {percentage:.2f}%\")\n\n# Example usage:\ncalculate_outliers_percentage(df)","metadata":{"id":"SQnLloVzbhVx","outputId":"5d09ff35-4fe3-4dd2-8563-ebf136c02768"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def handle_outliers(df):\n    for column in numeric_cols:\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        # Capping\n        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n\n    return df\n\n# Apply to both training and test datasets\ndf = handle_outliers(df)","metadata":{"id":"tsVcchH4cCYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_outliers_percentage(df)","metadata":{"id":"0l3ZXlQWcPbC","outputId":"2affda48-d70a-42db-a2f2-3760f8eb1d36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.violinplot(df)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"id":"wKDRKnd27K1V","outputId":"ab02587f-8e55-4388-c09d-00ff047f76eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by 'Continent' and aggregate population data\npopulation_agg = df.groupby('Continent')['2022 Population'].agg(['sum', 'mean', 'median', 'std']).reset_index()\n\n# Rename columns for clarity\npopulation_agg.columns = ['Continent', 'Total Population', 'Mean Population', 'Median Population', 'Population Std Dev']\n\n# Display the aggregated table\npopulation_agg.sort_values(by='Total Population', ascending=False)","metadata":{"id":"79Yns2-Eg4kR","outputId":"7dfdd4a4-d511-4a2a-d465-d3e475d1ec99"},"execution_count":null,"outputs":[]}]}